{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNTnqpGvQdibGlaxV7nK1sg",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TsienJin/SC3000-Group-Project/blob/main/TJ_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OrS4hCnwGXVD"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Observation = namedtuple(\"observation\", (\"cartPos\", \"cartVel\", \"poleAngle\", \"poleVel\"))\n",
    "\n",
    "Environment = namedtuple(\"environment\", (\"observation\", \"reward\", \"isDone\", \"isTruncated\"))\n",
    "\n",
    "# Referenced as \"experience\" in the DQN paper\n",
    "Record = namedtuple(\"record\", (\"state\", \"action\", \"nextState\", \"reward\"))\n"
   ],
   "metadata": {
    "id": "x2mdmOmtG2lk"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class ParseEnvironment:\n",
    "    def __init__(self, environment:[float], reward:float=None, isDone:bool=None, isTruncated:bool=None, *args):\n",
    "        self.cartPos = environment[0]\n",
    "        self.cartVel = environment[1]\n",
    "        self.poleAngle = environment[2]\n",
    "        self.poleVel = environment[3]\n",
    "\n",
    "        self.reward = reward\n",
    "        self.isDone = isDone\n",
    "        self.isTruncated = isTruncated\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"cPOS: {self.cartPos}\\ncVEL: {self.cartVel}\\npANG: {self.poleAngle}\\npVEL: {self.poleVel}\\nreward: {self.reward}\\nisDone: {self.isDone}\\nisTruncated: {self.isTruncated}\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "    def toObservation(self) -> Observation:\n",
    "        return Observation(self.cartPos, self.cartVel, self.poleAngle, self.poleVel)\n",
    "\n",
    "    def toTensor(self):\n",
    "        return torch.FloatTensor((self.cartPos, self.cartVel, self.poleAngle, self.poleVel))\n",
    "\n",
    "    def toFloat32(self) -> [np.float32]:\n",
    "        return np.array([self.cartPos, self.cartVel, self.poleAngle, self.poleVel], type=np.float32)\n",
    "\n",
    "    def toEnvironment(self) -> Environment:\n",
    "        return Environment(self.toObservation(), self.reward, self.isDone, self.isTruncated)\n",
    "\n",
    "\n",
    "class ParseRecord:\n",
    "    def __init__(self, state: ParseEnvironment, action: int, nextState: ParseEnvironment, reward: float):\n",
    "        assert action in [0, 1]\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.nextState = nextState\n",
    "        self.reward = reward\n",
    "\n",
    "    def toRecord(self) -> Record:\n",
    "        return Record(self.state, self.action, self.nextState, self.reward)\n"
   ],
   "metadata": {
    "id": "lMli804DHJlN"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class Memory:\n",
    "    def __init__(self, maxCapacity:int=10000):\n",
    "        self.cap = maxCapacity\n",
    "        self.memory = deque([], maxlen=maxCapacity)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"\"\"Memory() capacity [{self.__len__}/{self.cap}]\"\"\"\n",
    "\n",
    "    def push(self, record:ParseRecord) -> None:\n",
    "        self.memory.append(record)\n",
    "\n",
    "    def sample(self, size:int) -> [ParseRecord]:\n",
    "        assert size>0\n",
    "        return random.sample(self.memory, size)\n"
   ],
   "metadata": {
    "id": "cZ5YslyxG5eQ"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_obsv: int, n_actions: int, n_layer: int = 1, n_layerSize: int = 6,\n",
    "                 learningRate: float = 0.0001, gamma: float = 0.95,\n",
    "                 expDecay: float = 0.999, expMin: float = 0.001, expMax: float = 1.0,\n",
    "                 _device: str = \"cpu\",\n",
    "                 memory: Memory = Memory()):\n",
    "        \"\"\"\n",
    "\n",
    "        :param n_obsv: size of observation space\n",
    "        :param n_actions: size of action space\n",
    "        :param n_layer: number of hidden layers\n",
    "        :param n_layerSize: number of neurons per hidden layer\n",
    "        :param learningRate:\n",
    "        :param gamma: discount for future values\n",
    "        :param expDecay:\n",
    "        :param expMin:\n",
    "        :param expMax:\n",
    "        :param _device: defaults to \"cpu\"\n",
    "        \"\"\"\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Ensuring that values are proper\n",
    "        assert n_layer >= 0\n",
    "        assert n_obsv > 0\n",
    "        assert n_actions > 0\n",
    "        assert n_layerSize > 0\n",
    "        assert 0 < learningRate < 1\n",
    "        assert 0 < gamma < 1\n",
    "        assert 0 < expDecay < 1\n",
    "        assert 0 < expMin < 1\n",
    "        assert 0 < expMax <= 1\n",
    "\n",
    "        self.learningRate = learningRate\n",
    "        self.gamma = gamma\n",
    "        self.expDecay = expDecay\n",
    "        self.expMin = expMin\n",
    "        self.expMax = expMax\n",
    "\n",
    "        self.n_obsv = n_obsv\n",
    "        self.n_actions = n_actions\n",
    "        self.n_layer = n_layer\n",
    "        self.n_layerSize = n_layerSize\n",
    "\n",
    "        self.memory = memory\n",
    "\n",
    "        self.layers = nn.ModuleList(self.__createLayers())\n",
    "        self.optim = optim.Adam(self.parameters(), lr=self.learningRate)\n",
    "        self.crit = torch.nn.SmoothL1Loss()  # Huber loss\n",
    "\n",
    "        self.to(_device)\n",
    "\n",
    "    def __createLayers(self):\n",
    "        \"\"\"\n",
    "        Private method to generate neural network given the specified params in __init__()\n",
    "        :return: [nn.Linear()]\n",
    "        \"\"\"\n",
    "        # init layers starting with input shape to layer size\n",
    "        layers = [nn.Linear(self.n_obsv, self.n_layerSize)]\n",
    "\n",
    "        # creates more layers with specified layer size\n",
    "        for _ in range(self.n_layer):\n",
    "            layers.append(nn.Linear(self.n_layerSize, self.n_layerSize))\n",
    "\n",
    "        # adds final output layer\n",
    "        layers.append(nn.Linear(self.n_layerSize, self.n_actions))\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x:torch.float32) -> torch.float32:\n",
    "        \"\"\"\n",
    "        Processes the given state and returns a tensor with qValues for actions\n",
    "        :param x: <torch.tensor> with shape (1,4) and type float32 | State of current observation as a tensor\n",
    "        :return: <torch.tensor> with shape (1,2) | Tensor of qValues\n",
    "        \"\"\"\n",
    "        assert (x.dim() == torch.randn(4).dim())\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return x"
   ],
   "metadata": {
    "id": "AiW2eKE1HFk_"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class Agent:\n",
    "    # Number of episodes\n",
    "    MAX_EP = 1_000_000\n",
    "\n",
    "    # Q Value vals\n",
    "    DISCOUNT = 0.9\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    # Epsilon GREEDY vals\n",
    "    EPS = 0.9999\n",
    "    EPS_DECAY = 0.999\n",
    "    EPS_MIN = 0.05\n",
    "    EPS_MAX = 1.0\n",
    "\n",
    "    # Memory vals\n",
    "    MEM_SIZE = 50_000\n",
    "    MIN_MEM_SIZE = 1_000\n",
    "    MEM_BATCH = 200\n",
    "    TARGET_UPDATE_FREQ = 75\n",
    "\n",
    "    def __init__(self, maxEp:int=10_000, env=gym.make(\"CartPole-v1\")):\n",
    "\n",
    "        # Bootstrapping to maintain stability of prediction\n",
    "        self.memory = Memory(maxCapacity=self.MEM_SIZE)\n",
    "        self.model = DQN(n_obsv=4, n_actions=2, n_layer=10, n_layerSize=10,learningRate=self.LEARNING_RATE, memory=self.memory)  # updates every iteration\n",
    "        self.targetModel = deepcopy(self.model)  # updates only once threshold has been reached\n",
    "\n",
    "        # Setting individual stats for the environment to run\n",
    "        self.maxEpisode = maxEp\n",
    "        self.env = env\n",
    "        self.episodeCounter = 0\n",
    "        self.totalReward = 0\n",
    "\n",
    "    def __printStats(self):\n",
    "        print(f\"EP: {self.EPS:.3f} | MEM: {len(self.memory)} | EP: {self.episodeCounter} | AVG: {self.totalReward/self.episodeCounter:.5f}\")\n",
    "\n",
    "    def predict(self, environment:ParseEnvironment) -> int:\n",
    "        if self.EPS < self.EPS_MIN:\n",
    "            res = self.targetModel.forward(environment.toTensor())\n",
    "            return torch.argmax(res).detach().numpy()\n",
    "        else:\n",
    "            # print(\"USING RANDOM\")\n",
    "            self.EPS = self.EPS * self.EPS_DECAY\n",
    "            return random.randint(0,1)\n",
    "\n",
    "\n",
    "    def getMaxQ(self, environment:ParseEnvironment) -> torch.tensor:\n",
    "        res = self.targetModel.forward(environment.toTensor())\n",
    "        return res.clone().detach().numpy()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.MIN_MEM_SIZE:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(size=self.MEM_BATCH)\n",
    "\n",
    "        allStates = np.array([record.state for record in batch])  # need to check if this works; no intellisense\n",
    "        predicted = [self.getMaxQ(record.state) for record in batch]\n",
    "        predictedNew = [self.getMaxQ(record.nextState) for record in batch]\n",
    "\n",
    "        oldValsToFit = []\n",
    "        valsToFit = []\n",
    "\n",
    "        for index, env in enumerate(batch):\n",
    "            maxFutureQ = np.max(self.getMaxQ(env.nextState))\n",
    "\n",
    "            if not env.state.isDone:\n",
    "                newQ = env.reward + self.DISCOUNT * maxFutureQ\n",
    "            else:\n",
    "                newQ = env.reward\n",
    "\n",
    "            oldFit = self.getMaxQ(env.state)\n",
    "            toFit = deepcopy(oldFit)\n",
    "            toFit[env.action] = (1-self.LEARNING_RATE)*oldFit[env.action] + self.LEARNING_RATE * newQ\n",
    "\n",
    "\n",
    "            oldValsToFit.append(oldFit)\n",
    "            valsToFit.append(toFit)\n",
    "\n",
    "        loss = self.model.crit(torch.tensor(np.array(oldValsToFit), requires_grad=True), torch.tensor(np.array(valsToFit), requires_grad=True))\n",
    "        self.model.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.model.optim.step()\n",
    "\n",
    "        if self.episodeCounter % self.TARGET_UPDATE_FREQ == 0:\n",
    "            self.targetModel.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        while self.episodeCounter < self.maxEpisode:\n",
    "            self.episodeCounter += 1\n",
    "            cReward = 0.0\n",
    "            curEnv = ParseEnvironment(self.env.reset()[0], reward=1.0, isDone=False, isTruncated=False)\n",
    "\n",
    "            while curEnv.isDone is not True:\n",
    "\n",
    "                # interact with env\n",
    "                action = self.predict(curEnv)\n",
    "                prevEnv = curEnv\n",
    "                curEnv = ParseEnvironment(*self.env.step(action))\n",
    "\n",
    "                # save record of what just happened\n",
    "                thisRecord = ParseRecord(prevEnv, action, curEnv, curEnv.reward)\n",
    "                self.memory.push(thisRecord)\n",
    "\n",
    "                # # train model\n",
    "                self.train()\n",
    "\n",
    "                # # update local variables\n",
    "                cReward += curEnv.reward\n",
    "                self.totalReward += curEnv.reward\n",
    "                self.__printStats()\n"
   ],
   "metadata": {
    "id": "hfM8SLMJHSnU",
    "outputId": "d9aac268-d00d-4e69-9713-e886de80f224",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "agent = Agent(maxEp=1000)\n",
    "agent.run()"
   ],
   "metadata": {
    "id": "pE6jOKKEHdXb",
    "outputId": "c8813492-ae85-41da-bca8-efed4942379e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-69e6d3135a5f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAgent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmaxEp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-11-58c3c732f70f>\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     92\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepisodeCounter\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m             \u001B[0mcReward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 94\u001B[0;31m             \u001B[0mcurEnv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mParseEnvironment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1.0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0misDone\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0misTruncated\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0mcurEnv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misDone\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-19-d8db9ca1fdc0>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, environment, reward, isDone, isTruncated, *args)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mParseEnvironment\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0misDone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mbool\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0misTruncated\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mbool\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcartPos\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcartVel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpoleAngle\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: invalid index to scalar variable."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "K5sGUAI3Hfe7"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
